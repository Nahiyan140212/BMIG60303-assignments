{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LbMzg6gDaA7"
      },
      "source": [
        "# Assignment 06 - CNN for Binary Drug Detection\n",
        "## BMIG60030 - Fall 2025\n",
        "\n",
        "This notebook implements a Convolutional Neural Network for binary classification of drug mentions in clinical text using the MIMIC-Ext-DrugDetection dataset.\n",
        "\n",
        "**Binary Classification Task**: Detect whether a sentence contains any drug mention (has_drug: 0 or 1)\n",
        "\n",
        "**Dataset**: MIMIC-Ext-DrugDetection - Using train.csv (804) and val.csv (806) with labels\n",
        "\n",
        "**Optimized for**: Google Cloud A100 GPU\n",
        "\n",
        "**Note**: test.csv is not used as it doesn't contain output labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pq4VyDzKDb4z",
        "outputId": "ddfb6d98-5c4f-48cf-abeb-dee14c30dfa8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cA-eQwOsDaA9"
      },
      "source": [
        "## GPU Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFlB1QRbDaA9",
        "outputId": "b6f93325-0c7e-4d51-f4ac-7d53ada7fa85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 GPU(s):\n",
            "  GPU 0: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
            "\n",
            "Mixed precision enabled: mixed_float16\n",
            "Compute dtype: float16\n",
            "Variable dtype: float32\n",
            "\n",
            "XLA (Accelerated Linear Algebra) enabled for faster computation\n",
            "\n",
            "TensorFlow version: 2.19.0\n",
            "Built with CUDA: True\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability and configure TensorFlow\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# Enable memory growth to avoid OOM errors\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(f\"Found {len(gpus)} GPU(s):\")\n",
        "        for i, gpu in enumerate(gpus):\n",
        "            print(f\"  GPU {i}: {gpu}\")\n",
        "\n",
        "        # Enable mixed precision for A100 GPU (faster training)\n",
        "        from tensorflow.keras import mixed_precision\n",
        "        policy = mixed_precision.Policy('mixed_float16')\n",
        "        mixed_precision.set_global_policy(policy)\n",
        "        print(f\"\\nMixed precision enabled: {policy.name}\")\n",
        "        print(\"Compute dtype:\", policy.compute_dtype)\n",
        "        print(\"Variable dtype:\", policy.variable_dtype)\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "else:\n",
        "    print(\"No GPU found. Running on CPU.\")\n",
        "\n",
        "# Set XLA compilation for additional speedup\n",
        "tf.config.optimizer.set_jit(True)\n",
        "print(\"\\nXLA (Accelerated Linear Algebra) enabled for faster computation\")\n",
        "\n",
        "# Check TensorFlow version and build info\n",
        "print(f\"\\nTensorFlow version: {tf.__version__}\")\n",
        "print(f\"Built with CUDA: {tf.test.is_built_with_cuda()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "up4wmuuPDaA-"
      },
      "source": [
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQF4eoALDaA_",
        "outputId": "f54aea31-a5a2-4f25-edd9-3d8d47c0d1bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.4.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.0)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install gensim\n",
        "!pip install nltk\n",
        "!pip install pandas\n",
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtwgIe9jDaA_",
        "outputId": "3179a936-32aa-4fcb-bee7-2a87a55babdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All packages imported successfully!\n",
            "Available CPU cores: 12\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from random import shuffle\n",
        "import time\n",
        "from multiprocessing import cpu_count\n",
        "\n",
        "# Keras/TensorFlow imports\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "# NLP imports\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from gensim.models import KeyedVectors, Word2Vec\n",
        "import nltk\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "print(f\"All packages imported successfully!\")\n",
        "print(f\"Available CPU cores: {cpu_count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBiy1JiXDaA_"
      },
      "source": [
        "## Load MIMIC-Ext-DrugDetection Dataset\n",
        "\n",
        "**Note**: We only use train.csv and val.csv as test.csv doesn't contain labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ASGS6xyDaA_",
        "outputId": "317f61a4-211c-4e05-ef7f-91b22bdf8a34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded in 0.02 seconds\n",
            "\n",
            "Train set size: 804\n",
            "Validation set size: 806\n",
            "\n",
            "Class distribution in training set:\n",
            "has_drug\n",
            "True     402\n",
            "False    402\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Class distribution in validation set:\n",
            "has_drug\n",
            "False    403\n",
            "True     403\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Sample sentence from training set:\n",
            "Service: MEDICINE Allergies: shellfish derived Attending: ___ Chief Complaint: Nausea and vomiting, abdominal pain Major Surgical or Invasive Procedure: N/A History of Present Illness: Mr. ___ is a __\n"
          ]
        }
      ],
      "source": [
        "# Load the datasets (only train and val have labels)\n",
        "data_path = '/content/drive/MyDrive/Summer Lab Rotation/Data/MIMIC-Ext-DrugDetection'\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "train_df = pd.read_csv(os.path.join(data_path, 'train.csv'))\n",
        "val_df = pd.read_csv(os.path.join(data_path, 'val.csv'))\n",
        "# test.csv is NOT loaded as it doesn't contain labels\n",
        "\n",
        "print(f\"Loaded in {time.time() - start_time:.2f} seconds\")\n",
        "print(f\"\\nTrain set size: {len(train_df)}\")\n",
        "print(f\"Validation set size: {len(val_df)}\")\n",
        "print(f\"\\nClass distribution in training set:\")\n",
        "print(train_df['has_drug'].value_counts())\n",
        "print(f\"\\nClass distribution in validation set:\")\n",
        "print(val_df['has_drug'].value_counts())\n",
        "print(f\"\\nSample sentence from training set:\")\n",
        "print(train_df.iloc[0]['text'][:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ueb8IIH5DaA_",
        "outputId": "d44a6b13-083b-4867-b840-4f01ab32526c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 804 training samples\n",
            "Converted 806 validation samples\n",
            "\n",
            "Sample data point (label, text): (True, 'Service: MEDICINE Allergies: shellfish derived Attending: ___ Chief Complaint: Nausea and vomiting, ...')\n"
          ]
        }
      ],
      "source": [
        "# Convert to format expected by our functions: list of (label, text) tuples\n",
        "def df_to_dataset(df):\n",
        "    return [(row['has_drug'], str(row['text'])) for _, row in df.iterrows()]\n",
        "\n",
        "train_dataset = df_to_dataset(train_df)\n",
        "val_dataset = df_to_dataset(val_df)\n",
        "\n",
        "print(f\"Converted {len(train_dataset)} training samples\")\n",
        "print(f\"Converted {len(val_dataset)} validation samples\")\n",
        "print(f\"\\nSample data point (label, text): ({train_dataset[0][0]}, '{train_dataset[0][1][:100]}...')\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZCaFFkFDaBA"
      },
      "source": [
        "## Analyze Sentence Lengths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWf7lT0cDaBA",
        "outputId": "1900e12f-1a68-4fbe-820e-50252903598b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sentences: 1610\n",
            "Min length: 1\n",
            "Max length: 178\n",
            "Mean length: 19.79\n",
            "Median length: 15.00\n",
            "75th percentile: 25.00\n",
            "90th percentile: 40.00\n",
            "95th percentile: 51.55\n",
            "99th percentile: 88.91\n",
            "\n",
            "Sentences <= 30 tokens: 81.7%\n",
            "Sentences <= 50 tokens: 94.7%\n",
            "Sentences <= 75 tokens: 98.1%\n"
          ]
        }
      ],
      "source": [
        "# Analyze sentence lengths across train and validation datasets\n",
        "\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "all_lengths = [len(tokenizer.tokenize(text)) for _, text in train_dataset + val_dataset]\n",
        "\n",
        "\n",
        "print(f\"Total sentences: {len(all_lengths)}\")\n",
        "print(f\"Min length: {min(all_lengths)}\")\n",
        "print(f\"Max length: {max(all_lengths)}\")\n",
        "print(f\"Mean length: {np.mean(all_lengths):.2f}\")\n",
        "print(f\"Median length: {np.median(all_lengths):.2f}\")\n",
        "print(f\"75th percentile: {np.percentile(all_lengths, 75):.2f}\")\n",
        "print(f\"90th percentile: {np.percentile(all_lengths, 90):.2f}\")\n",
        "print(f\"95th percentile: {np.percentile(all_lengths, 95):.2f}\")\n",
        "print(f\"99th percentile: {np.percentile(all_lengths, 99):.2f}\")\n",
        "\n",
        "print(f\"\\nSentences <= 30 tokens: {sum(1 for l in all_lengths if l <= 30)/len(all_lengths)*100:.1f}%\")\n",
        "print(f\"Sentences <= 50 tokens: {sum(1 for l in all_lengths if l <= 50)/len(all_lengths)*100:.1f}%\")\n",
        "print(f\"Sentences <= 75 tokens: {sum(1 for l in all_lengths if l <= 75)/len(all_lengths)*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeSQSUOTDaBA"
      },
      "source": [
        "## Build Custom Word2Vec from MIMIC Dataset\n",
        "\n",
        "\n",
        "**GPU Optimization**: Using all CPU cores for parallel Word2Vec training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJyd8rMhDaBA",
        "outputId": "1c0a6d6f-76ca-41b9-b8c4-02d01cd47c9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing texts...\n",
            "Tokenization completed in 0.08 seconds\n",
            "\n",
            "Training custom Word2Vec on 1610 sentences...\n",
            "Total tokens: 31,859\n",
            "\n",
            "Word2Vec training completed in 1.46 seconds\n",
            "MIMIC Word2Vec vocabulary size: 2,110\n",
            "\n",
            "Sample medical/drug terms in vocabulary:\n",
            "['with', 'history', 'abuse', 'patient', 'heroin', 'drug', 'cocaine', 'past', 'pain', 'blood', 'ivdu', 'that', 'from', 'last', 'polysubstance', 'disorder', 'medical', 'substance', 'this', 'alcohol']\n"
          ]
        }
      ],
      "source": [
        "# Combine train and val datasets for training Word2Vec\n",
        "all_texts = [text for _, text in train_dataset + val_dataset]\n",
        "\n",
        "# Tokenize all texts\n",
        "print(\"Tokenizing texts...\")\n",
        "start_time = time.time()\n",
        "tokenized_texts = [tokenizer.tokenize(text.lower()) for text in all_texts]\n",
        "print(f\"Tokenization completed in {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "print(f\"\\nTraining custom Word2Vec on {len(tokenized_texts)} sentences...\")\n",
        "print(f\"Total tokens: {sum(len(sent) for sent in tokenized_texts):,}\")\n",
        "\n",
        "start_time = time.time()\n",
        "mimic_w2v_model = Word2Vec(\n",
        "    sentences=tokenized_texts,\n",
        "    vector_size=300,\n",
        "    window=5,\n",
        "    min_count=2,\n",
        "    sg=1,\n",
        "    workers=cpu_count(),  # Use all CPU cores\n",
        "    epochs=10,\n",
        "    negative=10,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "mimic_word_vectors = mimic_w2v_model.wv\n",
        "\n",
        "print(f\"\\nWord2Vec training completed in {training_time:.2f} seconds\")\n",
        "print(f\"MIMIC Word2Vec vocabulary size: {len(mimic_word_vectors):,}\")\n",
        "print(f\"\\nSample medical/drug terms in vocabulary:\")\n",
        "sample_words = [w for w in mimic_word_vectors.index_to_key[:100] if len(w) > 3]\n",
        "print(sample_words[:20])\n",
        "\n",
        "embedding_dims = 300"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZejqP2VDaBA"
      },
      "source": [
        "## Helper Functions (GPU-Optimized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48TDvETXDaBA",
        "outputId": "068e8a2e-bf2a-4765-bff2-3f0f035eb161"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Helper functions defined successfully!\n"
          ]
        }
      ],
      "source": [
        "def tokenize_and_vectorize(dataset, word_vectors):\n",
        "    \"\"\"\n",
        "    Tokenize text and convert tokens to word vectors.\n",
        "    Optimized for speed with list comprehensions.\n",
        "    \"\"\"\n",
        "    tokenizer = TreebankWordTokenizer()\n",
        "    vectorized_data = []\n",
        "\n",
        "    for sample in dataset:\n",
        "        tokens = tokenizer.tokenize(sample[1].lower())\n",
        "        # Use list comprehension for speed\n",
        "        sample_vecs = [word_vectors[token] for token in tokens if token in word_vectors]\n",
        "        vectorized_data.append(sample_vecs)\n",
        "\n",
        "    return vectorized_data\n",
        "\n",
        "\n",
        "def collect_expected(dataset):\n",
        "    \"\"\"Extract labels from dataset.\"\"\"\n",
        "    return [sample[0] for sample in dataset]\n",
        "\n",
        "\n",
        "def pad_trunc(data, maxlen, embedding_dims):\n",
        "    \"\"\"\n",
        "    Pad with zero vectors or truncate to maxlen.\n",
        "    Vectorized for speed.\n",
        "    \"\"\"\n",
        "    new_data = []\n",
        "    zero_vector = np.zeros(embedding_dims, dtype=np.float32)  # Use numpy for speed\n",
        "\n",
        "    for sample in data:\n",
        "        if len(sample) > maxlen:\n",
        "            temp = sample[:maxlen]\n",
        "        elif len(sample) < maxlen:\n",
        "            # Pad with zero vectors\n",
        "            temp = sample + [zero_vector] * (maxlen - len(sample))\n",
        "        else:\n",
        "            temp = sample\n",
        "        new_data.append(temp)\n",
        "\n",
        "    return new_data\n",
        "\n",
        "\n",
        "def prepare_data(train_dataset, val_dataset, word_vectors, maxlen, embedding_dims):\n",
        "    \"\"\"\n",
        "    Complete data preparation pipeline.\n",
        "    Returns data in float32 for GPU efficiency.\n",
        "\n",
        "    Args:\n",
        "        train_dataset: Training data (list of (label, text) tuples)\n",
        "        val_dataset: Validation data (used for both validation and testing)\n",
        "        word_vectors: Word embeddings (Word2Vec or GloVe)\n",
        "        maxlen: Maximum sequence length\n",
        "        embedding_dims: Dimension of word embeddings\n",
        "\n",
        "    Returns:\n",
        "        x_train, y_train, x_val, y_val: Prepared arrays\n",
        "    \"\"\"\n",
        "    print(\"Preparing data...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Vectorize\n",
        "    x_train = tokenize_and_vectorize(train_dataset, word_vectors)\n",
        "    x_val = tokenize_and_vectorize(val_dataset, word_vectors)\n",
        "\n",
        "    # Extract labels\n",
        "    y_train = collect_expected(train_dataset)\n",
        "    y_val = collect_expected(val_dataset)\n",
        "\n",
        "    # Pad/truncate\n",
        "    x_train = pad_trunc(x_train, maxlen, embedding_dims)\n",
        "    x_val = pad_trunc(x_val, maxlen, embedding_dims)\n",
        "\n",
        "    # Reshape to numpy arrays with float32 for GPU efficiency\n",
        "    x_train = np.array(x_train, dtype=np.float32).reshape(len(x_train), maxlen, embedding_dims)\n",
        "    y_train = np.array(y_train, dtype=np.float32)\n",
        "    x_val = np.array(x_val, dtype=np.float32).reshape(len(x_val), maxlen, embedding_dims)\n",
        "    y_val = np.array(y_val, dtype=np.float32)\n",
        "\n",
        "    print(f\"Data preparation completed in {time.time() - start_time:.2f} seconds\")\n",
        "    print(f\"Train shape: {x_train.shape}, Val shape: {x_val.shape}\")\n",
        "\n",
        "    return x_train, y_train, x_val, y_val\n",
        "\n",
        "\n",
        "def build_cnn_model(maxlen, embedding_dims, filters=250, kernel_size=3, hidden_dims=250):\n",
        "    \"\"\"\n",
        "    Build CNN model for binary classification.\n",
        "    Optimized for GPU with mixed precision.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    # Convolutional layer with ReLU activation\n",
        "    model.add(Conv1D(filters,\n",
        "                     kernel_size,\n",
        "                     padding='valid',\n",
        "                     activation='relu',\n",
        "                     strides=1,\n",
        "                     input_shape=(maxlen, embedding_dims)))\n",
        "\n",
        "    # Global max pooling\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "\n",
        "    # Dense hidden layer\n",
        "    model.add(Dense(hidden_dims))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    # Output layer - sigmoid for binary classification\n",
        "    # Use float32 for final layer (required for mixed precision)\n",
        "    model.add(Dense(1, dtype='float32'))\n",
        "    model.add(Activation('sigmoid', dtype='float32'))\n",
        "\n",
        "    # Compile with Adam optimizer and binary crossentropy\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_callbacks(model_name='best_model'):\n",
        "    \"\"\"\n",
        "    Create training callbacks for better GPU utilization and performance.\n",
        "    \"\"\"\n",
        "    callbacks = [\n",
        "        # Early stopping to prevent overfitting\n",
        "        EarlyStopping(\n",
        "            monitor='val_accuracy',\n",
        "            patience=3,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        # Reduce learning rate when plateau\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=2,\n",
        "            min_lr=1e-7,\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "    return callbacks\n",
        "\n",
        "\n",
        "print(\"Helper functions defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qP1KbI6eDaBA"
      },
      "source": [
        "---\n",
        "# PART 1: Experiment with Different maxlen Values\n",
        "\n",
        "The `maxlen` parameter determines:\n",
        "- Input layer size of the CNN\n",
        "- How many tokens from long documents are kept (truncation)\n",
        "- How many zero vectors are added to short documents (padding)\n",
        "\n",
        "Based on our sentence length analysis:\n",
        "- Mean: ~20 tokens, Median: ~15 tokens  \n",
        "- 90th percentile: ~40 tokens, 95th percentile: ~53 tokens\n",
        "\n",
        "We'll test values that make sense for this distribution.\n",
        "\n",
        "**GPU Optimization**: Using larger batch size (128) to fully utilize A100 GPU memory and throughput."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "q07s47mmDaBB"
      },
      "outputs": [],
      "source": [
        "# Set batch size optimized for A100 GPU\n",
        "batch_size = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8OyRtJaDaBB"
      },
      "source": [
        "### maxlen = 30\n",
        "Testing around the 75th percentile - captures most sentences with minimal padding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        },
        "id": "z1BMHuFVDaBB",
        "outputId": "9baf20da-393d-49a2-b611-111d3ab46e67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing data...\n",
            "Data preparation completed in 0.16 seconds\n",
            "Train shape: (804, 30, 300), Val shape: (806, 30, 300)\n",
            "\n",
            "Model architecture:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_10\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_10\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv1d_10 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m250\u001b[0m)        │       \u001b[38;5;34m225,250\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d_10         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_20 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m)            │        \u001b[38;5;34m62,750\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_20 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m251\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_21 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv1d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">225,250</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d_10         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">62,750</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">251</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m288,251\u001b[0m (1.10 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">288,251</span> (1.10 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m288,251\u001b[0m (1.10 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">288,251</span> (1.10 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model on GPU...\n",
            "Epoch 1/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 498ms/step - accuracy: 0.6150 - loss: 0.6401 - val_accuracy: 0.8573 - val_loss: 0.4222 - learning_rate: 0.0010\n",
            "Epoch 2/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8631 - loss: 0.4070 - val_accuracy: 0.8859 - val_loss: 0.3040 - learning_rate: 0.0010\n",
            "Epoch 3/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8901 - loss: 0.3031 - val_accuracy: 0.9007 - val_loss: 0.2662 - learning_rate: 0.0010\n",
            "Epoch 4/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8908 - loss: 0.2825 - val_accuracy: 0.8983 - val_loss: 0.2641 - learning_rate: 0.0010\n",
            "Epoch 5/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9088 - loss: 0.2333 - val_accuracy: 0.8995 - val_loss: 0.2727 - learning_rate: 0.0010\n",
            "Restoring model weights from the end of the best epoch: 3.\n",
            "\n",
            "*** Validation Accuracy with maxlen=30: 0.9007 ***\n",
            "Training time: 6.77s | Total time: 8.02s\n"
          ]
        }
      ],
      "source": [
        "# maxlen = 30 (around 75th percentile)\n",
        "maxlen_1 = 30\n",
        "epochs = 5  # Using 5 epochs initially\n",
        "\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "x_train, y_train, x_val, y_val = prepare_data(\n",
        "    train_dataset, val_dataset,\n",
        "    mimic_word_vectors, maxlen_1, embedding_dims\n",
        ")\n",
        "\n",
        "model_maxlen_30 = build_cnn_model(maxlen_1, embedding_dims)\n",
        "print(\"\\nModel architecture:\")\n",
        "model_maxlen_30.summary()\n",
        "\n",
        "print(\"\\nTraining model on GPU...\")\n",
        "train_start = time.time()\n",
        "history_maxlen_30 = model_maxlen_30.fit(\n",
        "    x_train, y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(x_val, y_val),\n",
        "    callbacks=create_callbacks('maxlen_30'),\n",
        "    verbose=1\n",
        ")\n",
        "train_time = time.time() - train_start\n",
        "\n",
        "# Evaluate on validation set (used as test set)\n",
        "val_loss_30, val_acc_30 = model_maxlen_30.evaluate(x_val, y_val, verbose=0)\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\n*** Validation Accuracy with maxlen=30: {val_acc_30:.4f} ***\")\n",
        "print(f\"Training time: {train_time:.2f}s | Total time: {total_time:.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWQM2DQIDaBB"
      },
      "source": [
        "###  maxlen = 50  \n",
        "Testing around the 95th percentile - captures almost all sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPXcqeBfDaBB",
        "outputId": "acbb3c58-d2f8-402a-92f2-20a0c612906f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing data...\n",
            "Data preparation completed in 0.18 seconds\n",
            "Train shape: (804, 50, 300), Val shape: (806, 50, 300)\n",
            "Training model on GPU...\n",
            "Epoch 1/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 522ms/step - accuracy: 0.6577 - loss: 0.6154 - val_accuracy: 0.8362 - val_loss: 0.4023 - learning_rate: 0.0010\n",
            "Epoch 2/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8457 - loss: 0.4031 - val_accuracy: 0.9007 - val_loss: 0.2850 - learning_rate: 0.0010\n",
            "Epoch 3/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8770 - loss: 0.3196 - val_accuracy: 0.9020 - val_loss: 0.2645 - learning_rate: 0.0010\n",
            "Epoch 4/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8915 - loss: 0.3038 - val_accuracy: 0.9082 - val_loss: 0.2493 - learning_rate: 0.0010\n",
            "Epoch 5/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9126 - loss: 0.2171 - val_accuracy: 0.9057 - val_loss: 0.2650 - learning_rate: 0.0010\n",
            "Restoring model weights from the end of the best epoch: 4.\n",
            "\n",
            "*** Validation Accuracy with maxlen=50: 0.9082 ***\n",
            "Training time: 7.01s | Total time: 8.33s\n"
          ]
        }
      ],
      "source": [
        "# maxlen = 50 (around 95th percentile)\n",
        "maxlen_2 = 50\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "x_train, y_train, x_val, y_val = prepare_data(\n",
        "    train_dataset, val_dataset,\n",
        "    mimic_word_vectors, maxlen_2, embedding_dims\n",
        ")\n",
        "\n",
        "model_maxlen_50 = build_cnn_model(maxlen_2, embedding_dims)\n",
        "\n",
        "print(\"Training model on GPU...\")\n",
        "train_start = time.time()\n",
        "history_maxlen_50 = model_maxlen_50.fit(\n",
        "    x_train, y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(x_val, y_val),\n",
        "    callbacks=create_callbacks('maxlen_50'),\n",
        "    verbose=1\n",
        ")\n",
        "train_time = time.time() - train_start\n",
        "\n",
        "val_loss_50, val_acc_50 = model_maxlen_50.evaluate(x_val, y_val, verbose=0)\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\n*** Validation Accuracy with maxlen=50: {val_acc_50:.4f} ***\")\n",
        "print(f\"Training time: {train_time:.2f}s | Total time: {total_time:.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCENKIFvDaBB"
      },
      "source": [
        "###  maxlen = 75\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vdtn1TtaDaBB",
        "outputId": "c93eb4df-c087-423b-f4fc-1691eac1658e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing data...\n",
            "Data preparation completed in 0.21 seconds\n",
            "Train shape: (804, 75, 300), Val shape: (806, 75, 300)\n",
            "Training model on GPU...\n",
            "Epoch 1/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 523ms/step - accuracy: 0.5747 - loss: 0.6406 - val_accuracy: 0.8610 - val_loss: 0.4130 - learning_rate: 0.0010\n",
            "Epoch 2/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8640 - loss: 0.4035 - val_accuracy: 0.9045 - val_loss: 0.2838 - learning_rate: 0.0010\n",
            "Epoch 3/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8971 - loss: 0.2903 - val_accuracy: 0.9057 - val_loss: 0.2583 - learning_rate: 0.0010\n",
            "Epoch 4/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8920 - loss: 0.2795 - val_accuracy: 0.9144 - val_loss: 0.2449 - learning_rate: 0.0010\n",
            "Epoch 5/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9096 - loss: 0.2358 - val_accuracy: 0.9132 - val_loss: 0.2448 - learning_rate: 0.0010\n",
            "Restoring model weights from the end of the best epoch: 4.\n",
            "\n",
            "*** Validation Accuracy with maxlen=75: 0.9144 ***\n",
            "Training time: 7.14s | Total time: 8.52s\n"
          ]
        }
      ],
      "source": [
        "# maxlen = 75 (captures ~98-99% of sentences)\n",
        "maxlen_3 = 75\n",
        "\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "x_train, y_train, x_val, y_val = prepare_data(\n",
        "    train_dataset, val_dataset,\n",
        "    mimic_word_vectors, maxlen_3, embedding_dims\n",
        ")\n",
        "\n",
        "model_maxlen_75 = build_cnn_model(maxlen_3, embedding_dims)\n",
        "\n",
        "print(\"Training model on GPU...\")\n",
        "train_start = time.time()\n",
        "history_maxlen_75 = model_maxlen_75.fit(\n",
        "    x_train, y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(x_val, y_val),\n",
        "    callbacks=create_callbacks('maxlen_75'),\n",
        "    verbose=1\n",
        ")\n",
        "train_time = time.time() - train_start\n",
        "\n",
        "val_loss_75, val_acc_75 = model_maxlen_75.evaluate(x_val, y_val, verbose=0)\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\n*** Validation Accuracy with maxlen=75: {val_acc_75:.4f} ***\")\n",
        "print(f\"Training time: {train_time:.2f}s | Total time: {total_time:.2f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtS6vQ7xDaBC",
        "outputId": "641814c0-bb99-46e7-88ca-38ae4b59320d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "maxlen=30:  Validation Accuracy = 0.9007\n",
            "maxlen=50:  Validation Accuracy = 0.9082\n",
            "maxlen=75:  Validation Accuracy = 0.9144\n",
            "\n",
            "*** BEST maxlen: 75 with accuracy 0.9144 ***\n"
          ]
        }
      ],
      "source": [
        "# Compare maxlen results\n",
        "\n",
        "print(f\"maxlen=30:  Validation Accuracy = {val_acc_30:.4f}\")\n",
        "print(f\"maxlen=50:  Validation Accuracy = {val_acc_50:.4f}\")\n",
        "print(f\"maxlen=75:  Validation Accuracy = {val_acc_75:.4f}\")\n",
        "\n",
        "# Select best maxlen\n",
        "best_maxlen_results = {\n",
        "    30: val_acc_30,\n",
        "    50: val_acc_50,\n",
        "    75: val_acc_75\n",
        "}\n",
        "best_maxlen = max(best_maxlen_results, key=best_maxlen_results.get)\n",
        "print(f\"\\n*** BEST maxlen: {best_maxlen} with accuracy {best_maxlen_results[best_maxlen]:.4f} ***\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvca3wfzDaBC"
      },
      "source": [
        "---\n",
        "# PART 2: Experiment with Different Epoch Values\n",
        "\n",
        "We'll use the best maxlen from Part 1 and experiment with different numbers of training epochs to find the optimal value (balancing learning vs. overfitting)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCT2zBemDaBC",
        "outputId": "de632b20-7d42-444d-cbe8-8bdcf2bf3698"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using maxlen = 75 for epoch experiments\n",
            "Preparing data...\n",
            "Data preparation completed in 0.19 seconds\n",
            "Train shape: (804, 75, 300), Val shape: (806, 75, 300)\n"
          ]
        }
      ],
      "source": [
        "# Use best maxlen from Part 1\n",
        "maxlen_final = best_maxlen\n",
        "print(f\"Using maxlen = {maxlen_final} for epoch experiments\")\n",
        "\n",
        "# Prepare data once with best maxlen\n",
        "x_train, y_train, x_val, y_val = prepare_data(\n",
        "    train_dataset, val_dataset,\n",
        "    mimic_word_vectors, maxlen_final, embedding_dims\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjg8CLcNDaBC"
      },
      "source": [
        "###  epochs = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcDd9SFEDaBC",
        "outputId": "c06d84f3-851d-47fc-fb16-efd4ba233fc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 521ms/step - accuracy: 0.6664 - loss: 0.6103 - val_accuracy: 0.8164 - val_loss: 0.4024 - learning_rate: 0.0010\n",
            "Epoch 2/3\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8263 - loss: 0.4357 - val_accuracy: 0.9032 - val_loss: 0.2886 - learning_rate: 0.0010\n",
            "Epoch 3/3\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8806 - loss: 0.3177 - val_accuracy: 0.8958 - val_loss: 0.2805 - learning_rate: 0.0010\n",
            "Restoring model weights from the end of the best epoch: 2.\n",
            "\n",
            "Final Validation Accuracy: 0.8958\n",
            "*** Validation Accuracy with epochs=3: 0.9032 ***\n",
            "Training time: 6.81s\n"
          ]
        }
      ],
      "source": [
        "epochs_1 = 3\n",
        "\"\"\n",
        "train_start = time.time()\n",
        "model_epochs_3 = build_cnn_model(maxlen_final, embedding_dims)\n",
        "history_epochs_3 = model_epochs_3.fit(\n",
        "    x_train, y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs_1,\n",
        "    validation_data=(x_val, y_val),\n",
        "    callbacks=create_callbacks('epochs_3'),\n",
        "    verbose=1\n",
        ")\n",
        "train_time = time.time() - train_start\n",
        "\n",
        "val_loss_e3, val_acc_e3 = model_epochs_3.evaluate(x_val, y_val, verbose=0)\n",
        "final_val_acc_e3 = history_epochs_3.history['val_accuracy'][-1]\n",
        "\n",
        "print(f\"\\nFinal Validation Accuracy: {final_val_acc_e3:.4f}\")\n",
        "print(f\"*** Validation Accuracy with epochs=3: {val_acc_e3:.4f} ***\")\n",
        "print(f\"Training time: {train_time:.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7t4mOTEDaBC"
      },
      "source": [
        "###  epochs = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-y4SnYoQDaBD",
        "outputId": "2d934213-8494-4888-8946-a56ec8fdd126"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 527ms/step - accuracy: 0.6278 - loss: 0.6223 - val_accuracy: 0.8772 - val_loss: 0.4126 - learning_rate: 0.0010\n",
            "Epoch 2/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8659 - loss: 0.4086 - val_accuracy: 0.9032 - val_loss: 0.2971 - learning_rate: 0.0010\n",
            "Epoch 3/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8818 - loss: 0.3105 - val_accuracy: 0.8921 - val_loss: 0.2875 - learning_rate: 0.0010\n",
            "Epoch 4/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8927 - loss: 0.2826 - val_accuracy: 0.9107 - val_loss: 0.2506 - learning_rate: 0.0010\n",
            "Epoch 5/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9134 - loss: 0.2227 - val_accuracy: 0.9032 - val_loss: 0.2575 - learning_rate: 0.0010\n",
            "Restoring model weights from the end of the best epoch: 4.\n",
            "\n",
            "Final Validation Accuracy: 0.9032\n",
            "*** Validation Accuracy with epochs=5: 0.9107 ***\n",
            "Training time: 7.14s\n"
          ]
        }
      ],
      "source": [
        "epochs_2 = 5\n",
        "\n",
        "\n",
        "\n",
        "train_start = time.time()\n",
        "model_epochs_5 = build_cnn_model(maxlen_final, embedding_dims)\n",
        "history_epochs_5 = model_epochs_5.fit(\n",
        "    x_train, y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs_2,\n",
        "    validation_data=(x_val, y_val),\n",
        "    callbacks=create_callbacks('epochs_5'),\n",
        "    verbose=1\n",
        ")\n",
        "train_time = time.time() - train_start\n",
        "\n",
        "val_loss_e5, val_acc_e5 = model_epochs_5.evaluate(x_val, y_val, verbose=0)\n",
        "final_val_acc_e5 = history_epochs_5.history['val_accuracy'][-1]\n",
        "\n",
        "print(f\"\\nFinal Validation Accuracy: {final_val_acc_e5:.4f}\")\n",
        "print(f\"*** Validation Accuracy with epochs=5: {val_acc_e5:.4f} ***\")\n",
        "print(f\"Training time: {train_time:.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VHu6HbtDaBD"
      },
      "source": [
        "### epochs = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8YH64uDDaBD",
        "outputId": "63f97870-a53c-4077-fd88-1d527589708c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 525ms/step - accuracy: 0.6452 - loss: 0.6208 - val_accuracy: 0.8859 - val_loss: 0.4142 - learning_rate: 0.0010\n",
            "Epoch 2/8\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8783 - loss: 0.4019 - val_accuracy: 0.8797 - val_loss: 0.3068 - learning_rate: 0.0010\n",
            "Epoch 3/8\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8829 - loss: 0.3264 - val_accuracy: 0.9082 - val_loss: 0.2635 - learning_rate: 0.0010\n",
            "Epoch 4/8\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8993 - loss: 0.2653 - val_accuracy: 0.9094 - val_loss: 0.2498 - learning_rate: 0.0010\n",
            "Epoch 5/8\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9137 - loss: 0.2532 - val_accuracy: 0.8945 - val_loss: 0.2724 - learning_rate: 0.0010\n",
            "Epoch 6/8\n",
            "\u001b[1m1/7\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9062 - loss: 0.2287\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9002 - loss: 0.2397 - val_accuracy: 0.8958 - val_loss: 0.2850 - learning_rate: 0.0010\n",
            "Epoch 7/8\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9184 - loss: 0.2225 - val_accuracy: 0.9156 - val_loss: 0.2534 - learning_rate: 5.0000e-04\n",
            "Epoch 8/8\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9182 - loss: 0.2081 - val_accuracy: 0.9119 - val_loss: 0.2481 - learning_rate: 5.0000e-04\n",
            "Restoring model weights from the end of the best epoch: 7.\n",
            "\n",
            "Final Validation Accuracy: 0.9119\n",
            "*** Validation Accuracy with epochs=8: 0.9156 ***\n",
            "Training time: 7.67s\n"
          ]
        }
      ],
      "source": [
        "epochs_3 = 8\n",
        "\n",
        "train_start = time.time()\n",
        "model_epochs_8 = build_cnn_model(maxlen_final, embedding_dims)\n",
        "history_epochs_8 = model_epochs_8.fit(\n",
        "    x_train, y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs_3,\n",
        "    validation_data=(x_val, y_val),\n",
        "    callbacks=create_callbacks('epochs_8'),\n",
        "    verbose=1\n",
        ")\n",
        "train_time = time.time() - train_start\n",
        "\n",
        "val_loss_e8, val_acc_e8 = model_epochs_8.evaluate(x_val, y_val, verbose=0)\n",
        "final_val_acc_e8 = history_epochs_8.history['val_accuracy'][-1]\n",
        "\n",
        "print(f\"\\nFinal Validation Accuracy: {final_val_acc_e8:.4f}\")\n",
        "print(f\"*** Validation Accuracy with epochs=8: {val_acc_e8:.4f} ***\")\n",
        "print(f\"Training time: {train_time:.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BYxwR18DaBD"
      },
      "source": [
        "###  epochs = 10 (checking for overfitting)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWrze_cODaBD",
        "outputId": "a4c925c7-0c77-4747-8647-ffc51a655b64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 528ms/step - accuracy: 0.5763 - loss: 0.6368 - val_accuracy: 0.8722 - val_loss: 0.4221 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8462 - loss: 0.4214 - val_accuracy: 0.8933 - val_loss: 0.3052 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8828 - loss: 0.3147 - val_accuracy: 0.9045 - val_loss: 0.2599 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8999 - loss: 0.2733 - val_accuracy: 0.8871 - val_loss: 0.2972 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m1/7\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8594 - loss: 0.3450\n",
            "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8969 - loss: 0.2913 - val_accuracy: 0.9020 - val_loss: 0.2667 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9119 - loss: 0.2308 - val_accuracy: 0.8945 - val_loss: 0.2808 - learning_rate: 5.0000e-04\n",
            "Epoch 6: early stopping\n",
            "Restoring model weights from the end of the best epoch: 3.\n",
            "\n",
            "Final Validation Accuracy: 0.8945\n",
            "*** Validation Accuracy with epochs=10: 0.9045 ***\n",
            "Training time: 7.35s\n"
          ]
        }
      ],
      "source": [
        "epochs_4 = 10\n",
        "\n",
        "train_start = time.time()\n",
        "model_epochs_10 = build_cnn_model(maxlen_final, embedding_dims)\n",
        "history_epochs_10 = model_epochs_10.fit(\n",
        "    x_train, y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs_4,\n",
        "    validation_data=(x_val, y_val),\n",
        "    callbacks=create_callbacks('epochs_10'),\n",
        "    verbose=1\n",
        ")\n",
        "train_time = time.time() - train_start\n",
        "\n",
        "val_loss_e10, val_acc_e10 = model_epochs_10.evaluate(x_val, y_val, verbose=0)\n",
        "final_val_acc_e10 = history_epochs_10.history['val_accuracy'][-1]\n",
        "\n",
        "print(f\"\\nFinal Validation Accuracy: {final_val_acc_e10:.4f}\")\n",
        "print(f\"*** Validation Accuracy with epochs=10: {val_acc_e10:.4f} ***\")\n",
        "print(f\"Training time: {train_time:.2f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wXSU5g_DaBD",
        "outputId": "dec38e95-d79f-4126-fec7-d51dfbc5dd59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "PART 2 RESULTS: Epochs Comparison\n",
            "============================================================\n",
            "epochs=3:  Validation Accuracy = 0.9032\n",
            "epochs=5:  Validation Accuracy = 0.9107\n",
            "epochs=8:  Validation Accuracy = 0.9156\n",
            "epochs=10: Validation Accuracy = 0.9045\n",
            "\n",
            "*** BEST epochs: 8 with validation accuracy 0.9156 ***\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Compare epoch results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PART 2 RESULTS: Epochs Comparison\")\n",
        "print(\"=\"*60)\n",
        "print(f\"epochs=3:  Validation Accuracy = {val_acc_e3:.4f}\")\n",
        "print(f\"epochs=5:  Validation Accuracy = {val_acc_e5:.4f}\")\n",
        "print(f\"epochs=8:  Validation Accuracy = {val_acc_e8:.4f}\")\n",
        "print(f\"epochs=10: Validation Accuracy = {val_acc_e10:.4f}\")\n",
        "\n",
        "# Select best epochs based on validation accuracy\n",
        "best_epochs_results = {\n",
        "    3: val_acc_e3,\n",
        "    5: val_acc_e5,\n",
        "    8: val_acc_e8,\n",
        "    10: val_acc_e10\n",
        "}\n",
        "best_epochs = max(best_epochs_results, key=best_epochs_results.get)\n",
        "print(f\"\\n*** BEST epochs: {best_epochs} with validation accuracy {best_epochs_results[best_epochs]:.4f} ***\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLcQQPCrDaBD"
      },
      "source": [
        "---\n",
        "# PART 4: Experiment with Different Word Embeddings\n",
        "\n",
        "We'll compare:\n",
        "1. **Custom MIMIC Word2Vec** (domain-specific, trained on clinical text) - already tested above\n",
        "2. **GloVe embeddings** (general-purpose, pre-trained on Wikipedia/web text)\n",
        "\n",
        "This comparison tests whether domain-specific embeddings outperform general-purpose embeddings for clinical NLP tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4825jtLDaBD"
      },
      "source": [
        "##  Load GloVe Embeddings\n",
        "\n",
        "**Download GloVe vectors from**: https://nlp.stanford.edu/projects/glove/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dfe9ce5",
        "outputId": "b94cc08a-62d5-4e2e-f24e-aee3308baf96"
      },
      "source": [
        "# Download GloVe vectors\n",
        "!wget https://nlp.stanford.edu/data/glove.6B.zip -O /tmp/glove.6B.zip\n",
        "!unzip /tmp/glove.6B.zip -d /tmp/"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-04 04:24:00--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2025-11-04 04:24:01--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘/tmp/glove.6B.zip’\n",
            "\n",
            "/tmp/glove.6B.zip   100%[===================>] 822.24M  4.99MB/s    in 2m 43s  \n",
            "\n",
            "2025-11-04 04:26:45 (5.04 MB/s) - ‘/tmp/glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  /tmp/glove.6B.zip\n",
            "  inflating: /tmp/glove.6B.50d.txt   \n",
            "  inflating: /tmp/glove.6B.100d.txt  \n",
            "  inflating: /tmp/glove.6B.200d.txt  \n",
            "  inflating: /tmp/glove.6B.300d.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_glove_vectors(glove_file, limit=None):\n",
        "    \"\"\"\n",
        "    Load GloVe vectors into Gensim KeyedVectors format.\n",
        "    Optimized for speed with numpy operations.\n",
        "    \"\"\"\n",
        "    print(f\"Loading GloVe vectors from {glove_file}...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    embeddings = {}\n",
        "    count = 0\n",
        "\n",
        "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.array(values[1:], dtype='float32')\n",
        "            embeddings[word] = vector\n",
        "\n",
        "            count += 1\n",
        "            if limit and count >= limit:\n",
        "                break\n",
        "\n",
        "            if count % 100000 == 0:\n",
        "                print(f\"  Loaded {count:,} vectors...\")\n",
        "\n",
        "    # Convert to KeyedVectors format\n",
        "    vector_size = len(next(iter(embeddings.values())))\n",
        "    kv = KeyedVectors(vector_size=vector_size)\n",
        "    kv.add_vectors(list(embeddings.keys()), list(embeddings.values()))\n",
        "\n",
        "    load_time = time.time() - start_time\n",
        "    print(f\"\\nLoaded {len(kv):,} GloVe vectors ({vector_size} dimensions) in {load_time:.2f} seconds\")\n",
        "    return kv"
      ],
      "metadata": {
        "id": "RgScfdjXIEEu"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgMLr_5MDaBD",
        "outputId": "e87fb0c0-847d-43fe-931e-ca7a4130ae01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading GloVe vectors from /tmp/glove.6B.300d.txt...\n",
            "  Loaded 100,000 vectors...\n",
            "  Loaded 200,000 vectors...\n",
            "  Loaded 300,000 vectors...\n",
            "  Loaded 400,000 vectors...\n",
            "\n",
            "Loaded 400,000 GloVe vectors (300 dimensions) in 25.25 seconds\n"
          ]
        }
      ],
      "source": [
        "# Load GloVe vectors\n",
        "# Update this path to where you have GloVe vectors downloaded\n",
        "glove_path = '/tmp/glove.6B.300d.txt'\n",
        "\n",
        "# Load all GloVe vectors (or set limit=200000)\n",
        "glove_word_vectors = load_glove_vectors(glove_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpCb2htIDaBE"
      },
      "source": [
        "## 4.2: Train and Evaluate with GloVe Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8H9P3veDaBE",
        "outputId": "e333c5a2-6607-4d0d-9811-16decef850a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using best configuration: maxlen=75, epochs=8\n",
            "Preparing data...\n",
            "Data preparation completed in 0.17 seconds\n",
            "Train shape: (804, 75, 300), Val shape: (806, 75, 300)\n",
            "\n",
            "Training model on GPU with GloVe embeddings...\n",
            "Epoch 1/8\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 526ms/step - accuracy: 0.5592 - loss: 0.7261 - val_accuracy: 0.8102 - val_loss: 0.4351 - learning_rate: 0.0010\n",
            "Epoch 2/8\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8396 - loss: 0.3932 - val_accuracy: 0.8871 - val_loss: 0.3248 - learning_rate: 0.0010\n",
            "Epoch 3/8\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - accuracy: 0.9104 - loss: 0.2496 - val_accuracy: 0.8921 - val_loss: 0.3015 - learning_rate: 0.0010\n",
            "Epoch 4/8\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9405 - loss: 0.1973 - val_accuracy: 0.9007 - val_loss: 0.2621 - learning_rate: 0.0010\n",
            "Epoch 5/8\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9442 - loss: 0.1503 - val_accuracy: 0.9132 - val_loss: 0.2685 - learning_rate: 0.0010\n",
            "Epoch 6/8\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9724 - loss: 0.1325 - val_accuracy: 0.9169 - val_loss: 0.2359 - learning_rate: 0.0010\n",
            "Epoch 7/8\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9853 - loss: 0.0692 - val_accuracy: 0.9194 - val_loss: 0.2341 - learning_rate: 0.0010\n",
            "Epoch 8/8\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9966 - loss: 0.0560 - val_accuracy: 0.9206 - val_loss: 0.2326 - learning_rate: 0.0010\n",
            "Restoring model weights from the end of the best epoch: 8.\n",
            "\n",
            "Final Validation Accuracy: 0.9206\n",
            "*** Validation Accuracy with GloVe: 0.9206 ***\n",
            "Training time: 8.06s | Total time: 9.33s\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(f\"Using best configuration: maxlen={maxlen_final}, epochs={best_epochs}\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Prepare data with GloVe vectors\n",
        "x_train_glove, y_train_glove, x_val_glove, y_val_glove = prepare_data(\n",
        "    train_dataset, val_dataset,\n",
        "    glove_word_vectors, maxlen_final, embedding_dims\n",
        ")\n",
        "\n",
        "# Train model\n",
        "print(\"\\nTraining model on GPU with GloVe embeddings...\")\n",
        "train_start = time.time()\n",
        "model_glove = build_cnn_model(maxlen_final, embedding_dims)\n",
        "history_glove = model_glove.fit(\n",
        "    x_train_glove, y_train_glove,\n",
        "    batch_size=batch_size,\n",
        "    epochs=best_epochs,\n",
        "    validation_data=(x_val_glove, y_val_glove),\n",
        "    callbacks=create_callbacks('glove'),\n",
        "    verbose=1\n",
        ")\n",
        "train_time = time.time() - train_start\n",
        "\n",
        "val_loss_glove, val_acc_glove = model_glove.evaluate(x_val_glove, y_val_glove, verbose=0)\n",
        "final_val_acc_glove = history_glove.history['val_accuracy'][-1]\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\nFinal Validation Accuracy: {final_val_acc_glove:.4f}\")\n",
        "print(f\"*** Validation Accuracy with GloVe: {val_acc_glove:.4f} ***\")\n",
        "print(f\"Training time: {train_time:.2f}s | Total time: {total_time:.2f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UT7D_0WhDaBE",
        "outputId": "cc2cc06f-701e-42e7-ec4f-e685481596a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom MIMIC Word2Vec: Validation Accuracy = 0.9156\n",
            "GloVe Embeddings:      Validation Accuracy = 0.9206\n",
            "\n",
            "*** BEST embedding: GloVe ***\n",
            "*** Best accuracy: 0.9206 ***\n",
            "*** Difference: 0.0050 (0.50%) ***\n"
          ]
        }
      ],
      "source": [
        "# Compare all embedding results\n",
        "\n",
        "# Get MIMIC Word2Vec result from best epochs experiment\n",
        "val_acc_mimic = best_epochs_results[best_epochs]\n",
        "\n",
        "print(f\"Custom MIMIC Word2Vec: Validation Accuracy = {val_acc_mimic:.4f}\")\n",
        "print(f\"GloVe Embeddings:      Validation Accuracy = {val_acc_glove:.4f}\")\n",
        "\n",
        "# Select best embedding\n",
        "embedding_results = {\n",
        "    'Custom MIMIC Word2Vec': val_acc_mimic,\n",
        "    'GloVe': val_acc_glove\n",
        "}\n",
        "best_embedding = max(embedding_results, key=embedding_results.get)\n",
        "improvement = abs(val_acc_mimic - val_acc_glove)\n",
        "\n",
        "print(f\"\\n*** BEST embedding: {best_embedding} ***\")\n",
        "print(f\"*** Best accuracy: {embedding_results[best_embedding]:.4f} ***\")\n",
        "print(f\"*** Difference: {improvement:.4f} ({improvement*100:.2f}%) ***\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baoQD8rGDaBE"
      },
      "source": [
        "---\n",
        "# Final Optimized Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee4dtcN5DaBE",
        "outputId": "10db1c33-c8a2-4b5e-c6b1-d59bfd5dcfe8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Hardware: Google Cloud A100 GPU\n",
            "  Mixed Precision Training: Enabled (float16/float32)\n",
            "  XLA Compilation: Enabled\n",
            "\n",
            "Dataset: MIMIC-Ext-DrugDetection\n",
            "  Train: 804 samples\n",
            "  Val/Test: 806 samples\n",
            "\n",
            "Optimal Hyperparameters:\n",
            "  maxlen:     75\n",
            "  epochs:     8\n",
            "  embeddings: GloVe\n",
            "  batch_size: 128 (GPU-optimized)\n",
            "  filters:    250\n",
            "  kernel_size: 3\n",
            "\n",
            "Best Results:\n",
            "  Validation Accuracy: 0.9206\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(f\"\\nHardware: Google Cloud A100 GPU\")\n",
        "print(f\"  Mixed Precision Training: Enabled (float16/float32)\")\n",
        "print(f\"  XLA Compilation: Enabled\")\n",
        "print(f\"\\nDataset: MIMIC-Ext-DrugDetection\")\n",
        "print(f\"  Train: {len(train_dataset)} samples\")\n",
        "print(f\"  Val/Test: {len(val_dataset)} samples\")\n",
        "print(f\"\\nOptimal Hyperparameters:\")\n",
        "print(f\"  maxlen:     {maxlen_final}\")\n",
        "print(f\"  epochs:     {best_epochs}\")\n",
        "print(f\"  embeddings: {best_embedding}\")\n",
        "print(f\"  batch_size: {batch_size} (GPU-optimized)\")\n",
        "print(f\"  filters:    250\")\n",
        "print(f\"  kernel_size: 3\")\n",
        "print(f\"\\nBest Results:\")\n",
        "print(f\"  Validation Accuracy: {max(embedding_results.values()):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldQsbeSvDaBE"
      },
      "source": [
        "## Test the Final Model on Sample Sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRRnVTp0DaBE",
        "outputId": "65519da8-0e45-46a9-a25b-199137ee9496"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NO DRUG      (score: 0.4178)  |  #Moderate normocytic anemia, stable #Diffuse marrow low signal in the spine #Splenomegaly #Elevated light chains in the serum, but with normal SPEP and UPEP -Suspect acutesuspect inflammatory block on his marrow from IVDU and infection -recommend follow-up with Hematology as an outpatient in ___ weeks\n",
            "DRUG MENTION (score: 0.9994)  |  Substance abuse/withdrawal - The patient had recently used both cocaine and heroin prior to admission.\n",
            "NO DRUG      (score: 0.1345)  |  The patient's nephew agreed and felt that patient should be clinically cleared as much as possible and then allowed to sit up to prevent aspiration..\n",
            "DRUG MENTION (score: 0.9963)  |  Pt is a ___ y.o male with h.o ETOH/opiate abuse, depression, HTN who presented with SI and ETOH intoxication/withdrawal.\n",
            "NO DRUG      (score: 0.1090)  |  The patient underwent rectal swabs for VRE preoperatively which was negative.\n",
            "NO DRUG      (score: 0.0120)  |  Status post splenectomy.\n",
            "NO DRUG      (score: 0.1474)  |  Findings were discussed with Dr. [ * * Last Name ( STitle ) * * ] at the time of dictation CULTURE DATA: [ * * <<DATE>> * * ]\n",
            "DRUG MENTION (score: 0.9215)  |  Per report, he took ten 1 mg pills of Xanax at 4pm on the day prior to admission and 2 1mg Xanax tablets again on the day of admission, 3 hours prior to presentation.\n",
            "NO DRUG      (score: 0.0049)  |  Alert, oriented x 3.\n",
            "DRUG MENTION (score: 0.8095)  |  Past medical history: IVDA Anxiety Depression\n"
          ]
        }
      ],
      "source": [
        "def predict_drug_mention(text, model, word_vectors, maxlen, embedding_dims):\n",
        "    \"\"\"\n",
        "    Predict whether a sentence contains a drug mention.\n",
        "    Returns: probability score (0-1)\n",
        "    \"\"\"\n",
        "    sample_dataset = [(0, text)]\n",
        "    vectorized = tokenize_and_vectorize(sample_dataset, word_vectors)\n",
        "    padded = pad_trunc(vectorized, maxlen, embedding_dims)\n",
        "    x = np.array(padded, dtype=np.float32).reshape(1, maxlen, embedding_dims)\n",
        "    prediction = model.predict(x, verbose=0)[0][0]\n",
        "    return prediction\n",
        "\n",
        "\n",
        "# Select the best model and vectors\n",
        "if best_embedding == 'Custom MIMIC Word2Vec':\n",
        "    if best_epochs == 3:\n",
        "        final_model = model_epochs_3\n",
        "    elif best_epochs == 5:\n",
        "        final_model = model_epochs_5\n",
        "    elif best_epochs == 8:\n",
        "        final_model = model_epochs_8\n",
        "    else:\n",
        "        final_model = model_epochs_10\n",
        "    final_vectors = mimic_word_vectors\n",
        "else:  # GloVe\n",
        "    final_model = model_glove\n",
        "    final_vectors = glove_word_vectors\n",
        "\n",
        "# Test on sample sentences\n",
        "test_sentences = [\n",
        "    \"#Moderate normocytic anemia, stable #Diffuse marrow low signal in the spine #Splenomegaly #Elevated light chains in the serum, but with normal SPEP and UPEP -Suspect acutesuspect inflammatory block on his marrow from IVDU and infection -recommend follow-up with Hematology as an outpatient in ___ weeks\",\n",
        "    \"Substance abuse/withdrawal - The patient had recently used both cocaine and heroin prior to admission.\",\n",
        "    \"The patient's nephew agreed and felt that patient should be clinically cleared as much as possible and then allowed to sit up to prevent aspiration..\",\n",
        "    \"Pt is a ___ y.o male with h.o ETOH/opiate abuse, depression, HTN who presented with SI and ETOH intoxication/withdrawal.\",\n",
        "    \"The patient underwent rectal swabs for VRE preoperatively which was negative.\",\n",
        "    \"Status post splenectomy.\",\n",
        "    \"Findings were discussed with Dr. [ * * Last Name ( STitle ) * * ] at the time of dictation CULTURE DATA: [ * * <<DATE>> * * ]\",\n",
        "    \"Per report, he took ten 1 mg pills of Xanax at 4pm on the day prior to admission and 2 1mg Xanax tablets again on the day of admission, 3 hours prior to presentation.\",\n",
        "    \"Alert, oriented x 3.\",\n",
        "    \"Past medical history: IVDA Anxiety Depression\"\n",
        "]\n",
        "\n",
        "#True, True, False, True, False, False, False, True, False, True\n",
        "\n",
        "for sent in test_sentences:\n",
        "    pred = predict_drug_mention(sent, final_model, final_vectors, maxlen_final, embedding_dims)\n",
        "    label = \"DRUG MENTION\" if pred > 0.5 else \"NO DRUG\"\n",
        "    print(f\"{label:12s} (score: {pred:.4f})  |  {sent}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "12ToB-6gPeaq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}